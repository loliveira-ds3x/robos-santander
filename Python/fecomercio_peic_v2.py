# -*- coding: utf-8 -*-
"""fecomercio_peic_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AVBPXKXonhBvcL2_9oKP9CkRohL4Vym
"""

import requests
import pandas as pd 
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from logging.config import dictConfig

start_url = "https://www.portaldaindustria.com.br/estatisticas/icei-indice-de-confianca-do-empresario-industrial/"
html_path = 'div.estatiscas-info-adicionais-edicao div:nth-of-type(1) > div:nth-of-type(6) div > a[href]'

r  = requests.get(start_url, allow_redirects=True)
data = r.text
soup = BeautifulSoup(data)
element = soup.select_one(html_path)
link_download = element['href']

print(element.prettify())
print(link_download)

###############  VARIAVEIS GLOBAIS  ###############
#INPUT FILE
crawler_name = 'fecomercio_peic'
start_url = "https://www.fecomercio.com.br/pesquisas/indice/peic"
html_path = 'div.boxDownload a[href]'
file_type = 'simple_url'

input_file = crawler_name + '.xls'
input_file_tab1 = 'Série Histórica'
index_columns = "A,B,C,D,F,G,H,J,K,L,M"
column_names = [ 'data','familias_endividadas_abs','familias_contas_em_atraso_abs','familias_sem_condicao_de_pagar_abs','familias_endividadas_perc','familias_contas_em_atraso_perc'
              ,'familias_sem_condicao_de_pagar_perc','prazo_comprometimento_renda_3_meses','prazo_comprometimento_renda_3_a_6_meses','prazo_comprometimento_renda_6_a_12_meses'
              ,'prazo_comprometimento_renda_mais_de_12_meses'
                ]

#OUTPUT FILE
output_file = crawler_name + '_hist.csv'


input_file2 = crawler_name + '_complex.xls'
output_file2 = crawler_name+'_hist_complex.csv'

LOGGING_CONFIG = {
    'version': 1,
    'loggers': {
        '': {  # root logger
            'level': 'NOTSET',
            'handlers': ['debug_console_handler', 'info_rotating_file_handler', 'error_file_handler', 'critical_mail_handler'],
        },
        'my.package': { 
            'level': 'WARNING',
            'propagate': False,
            'handlers': ['info_rotating_file_handler', 'error_file_handler' ],
        },
    },
    'handlers': {
        'debug_console_handler': {
            'level': 'DEBUG',
            'formatter': 'info',
            'class': 'logging.StreamHandler',
            'stream': 'ext://sys.stdout',
        },
        'info_rotating_file_handler': {
            'level': 'INFO',
            'formatter': 'info',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': crawler_name + '_info.log',
            'mode': 'a',
            'maxBytes': 1048576,
            'backupCount': 10
        },
        'error_file_handler': {
            'level': 'WARNING',
            'formatter': 'error',
            'class': 'logging.FileHandler',
            'filename': crawler_name + '_error.log',
            'mode': 'a',
        },
        'critical_mail_handler': {
            'level': 'CRITICAL',
            'formatter': 'error',
            'class': 'logging.handlers.SMTPHandler',
            'mailhost' : 'localhost',
            'fromaddr': 'monitoring@domain.com',
            'toaddrs': ['dev@domain.com', 'qa@domain.com'],
            'subject': 'Critical error with application name'
        }
    },
    'formatters': {
        'info': {
            'format': '%(asctime)s-%(levelname)s-%(name)s::%(module)s|%(lineno)s:: %(message)s'
        },
        'error': {
            'format': '%(asctime)s-%(levelname)s-%(name)s-%(process)d::%(module)s|%(lineno)s:: %(message)s'
        },
    },

}

###############  CONFIGURA LOG  ###############

def log (crawler_name):
    dictConfig(LOGGING_CONFIG)

class Get_file:
    def __init__(self, get_type):
        self.get_type = get_type

class Url_simple(Get_file):
    def __init__(self, start_url,html_path,input_file):
        super().__init__(file_type)
        self.start_url = start_url
        self.html_path = html_path
        self.input_file = input_file

    def get_file(self):
        #url da página do download  
        
        r  = requests.get(start_url, allow_redirects=True)
        try:
          logging.info('Iniciando download do arquivo')
          #url da página do download  
          r  = requests.get(start_url, allow_redirects=True)
          
          #o link muda todo mês; por isso faço get no link toda vez
          data = r.text
          soup = BeautifulSoup(data)
          element = soup.select_one(html_path)
          link_download = element['href']
          #request no link do download
          s = requests.get(link_download, allow_redirects=True)
        except:
          logging.error('Erro ao fazer o download')
        else:  
          logging.info('Download executado com sucesso')
          #abre o excel
          f = open(input_file, 'wb').write(s.content)
          return f

class Url_complex(Get_file):
    def __init__(self, start_url,html_path,input_file):
        super().__init__(file_type)
        self.start_url = start_url
        self.html_path = html_path
        self.input_file2 = input_file2

    def get_file(self):
        #url da página do download  
        
        r  = requests.get(start_url, allow_redirects=True)
        try:
          logging.info('Iniciando download do arquivo')
          #url da página do download  
          r  = requests.get(start_url, allow_redirects=True)
          
          #o link muda todo mês; por isso faço get no link toda vez
          data = r.text
          soup = BeautifulSoup(data)
          element = soup.select_one(html_path)
          link_download = element['href']
          #request no link do download
          s = requests.get(link_download, allow_redirects=True)
        except:
          logging.error('Erro ao fazer o download')
        else:  
          logging.info('Download executado com sucesso')
          #abre o excel
          f = open(input_file2, 'wb').write(s.content)
          return f

def File_etl(input_file_tab1,index_columns,column_names,output_file):         
  try:
    logging.info('Iniciando tratamentos do arquivo. Convertendo em csv.') 
                              
    #lê o excel, renomeia colunas e salva o csv
    df = pd.read_excel(input_file, input_file_tab1, usecols = index_columns)
    df.columns = column_names
    df.to_csv (output_file, index = False, header=True, sep='|')

  except:
    logging.error('Erro ao fazer tratamento do arquivo') 
  else:  
    logging.info('Csv gerado com sucesso')

def main():
  f_type = Get_file(file_type)
  print (f_type.get_type)

  if f_type.get_type == 'simple_url':
    file = Url_simple(start_url,html_path,input_file)
    file.get_file()
  elif f_type.get_type == 'complex_url':
    file = Url_complex(start_url,html_path,input_file2)
    file.get_file()
  else:
    logging.error('Erro nas classes')

  File_etl(input_file_tab1,index_columns,column_names,output_file)

log(crawler_name)
main()